{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_transition_prob_dict(transition_probs):\n",
    "    sorted_transition_probs = {}\n",
    "    for source in transition_probs:\n",
    "        sorted_targets = [k for k, v in sorted(transition_probs[source].items(), key=lambda item: item[1], reverse=True)]\n",
    "        sorted_transition_probs[source] = sorted_targets\n",
    "    return sorted_transition_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transition_prob_dict(paths, markov_order=1, min_session_len=2):\n",
    "    transition_probs = {}; num_sessions = 0\n",
    "    for lnum, line in enumerate(tqdm(paths)):\n",
    "        session = line.strip().split(\" \")\n",
    "        N = len(session)\n",
    "        if N<min_session_len:\n",
    "            continue\n",
    "        for idx in range(N-markov_order):\n",
    "            src_ids = [int(session[idx+j]) for j in range(markov_order)]\n",
    "            trgt_id = int(session[idx+markov_order])\n",
    "            src_id = tuple(src_ids) if markov_order>=2 else src_ids[0]\n",
    "\n",
    "            if src_id in transition_probs:\n",
    "                if trgt_id in transition_probs[src_id]:\n",
    "                    transition_probs[src_id][trgt_id] += 1\n",
    "                else:\n",
    "                    transition_probs[src_id][trgt_id] = 1\n",
    "            else:\n",
    "                transition_probs[src_id] = {trgt_id: 1}\n",
    "        num_sessions+=1\n",
    "    return transition_probs, num_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transition_probs(paths, markov_order=1, min_session_len=2):\n",
    "    transition_probs, num_sessions = build_transition_prob_dict(paths, markov_order, min_session_len)\n",
    "    sorted_transition_probs = sort_transition_prob_dict(transition_probs)\n",
    "    return sorted_transition_probs, num_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, fname):\n",
    "    fmodel_out = open(fname,\"wb\")\n",
    "    pickle.dump(model, fmodel_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(fname):\n",
    "    model = pickle.load(open(fname,'rb'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_queries(paths, N_max = -1, markov_order=1, min_session_len=2):\n",
    "    '''\n",
    "    get pairs of source-target articles from a file containing reading sessions.\n",
    "    from a file containing sequences of pageview.\n",
    "    select all pairs of consecutive pageivews.\n",
    "    returns a list of tuples [(src,trg)], where src, trg are of type str.\n",
    "\n",
    "    get at most pairs from N_max sessions (default is -1 == all).\n",
    "    '''\n",
    "    queries = []\n",
    "    count=0\n",
    "\n",
    "    for line in paths:\n",
    "        session = line.strip().split(\" \")\n",
    "\n",
    "        N = len(session)\n",
    "        if N<min_session_len:\n",
    "            continue\n",
    "\n",
    "        for idx in range(N-markov_order):\n",
    "            src_ids = [int(session[idx+j]) for j in range(markov_order)]\n",
    "            trgt_id = int(session[idx+markov_order])\n",
    "            src_id = tuple(src_ids) if markov_order>=2 else src_ids[0]\n",
    "\n",
    "            queries.append((N, (src_id,trgt_id)))\n",
    "        count+=1\n",
    "        if count == N_max:\n",
    "            break\n",
    "    return queries, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queriesRanks(queries, transition_probs, markov_order=1):\n",
    "    '''\n",
    "    from a list of pairs (src,target)\n",
    "    - get the nearest neighbors of src via the first-order-markov model inferred from the training reader sessions\n",
    "    - check rank of trg among nearest neighbors\n",
    "    '''\n",
    "    lengthwise_rank_list = {}\n",
    "    rank_list = []\n",
    "    missing_sources = 0; missing_targets = 0\n",
    "    for sess_len, (pid_src,pid_trg) in queries:\n",
    "        try:\n",
    "            if type(pid_src) == tuple:\n",
    "                if markov_order == 1:\n",
    "                    sorted_targets = transition_probs[pid_src[-1]]\n",
    "                else:\n",
    "                    sorted_targets = transition_probs[pid_src]\n",
    "            else:\n",
    "                sorted_targets = transition_probs[pid_src]\n",
    "            rank = sorted_targets.index(pid_trg)+1\n",
    "        except KeyError:\n",
    "            rank = 1e6\n",
    "            missing_sources+=1\n",
    "        except ValueError:\n",
    "            rank = 1e6\n",
    "            missing_targets +=1\n",
    "\n",
    "        if sess_len not in lengthwise_rank_list:\n",
    "            lengthwise_rank_list[sess_len] = [rank]\n",
    "        else:\n",
    "            lengthwise_rank_list[sess_len].append(rank)\n",
    "\n",
    "        rank_list.append(rank)\n",
    "    print(f'#Missing-sources = {missing_sources}; #Missing-targets = {missing_targets}')\n",
    "    return np.array(rank_list), lengthwise_rank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_query_skip_status(query, trgt, model_list):\n",
    "    skip_query = False\n",
    "    for model in model_list:\n",
    "        if query not in model:\n",
    "            skip_query = True\n",
    "            break\n",
    "        else:\n",
    "            if trgt not in model[query]:\n",
    "                skip_query = True\n",
    "                break\n",
    "    return skip_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queriesRanksNoMissing(queries, model_list, model, markov_order=1):\n",
    "    '''\n",
    "    from a list of pairs (src,target)\n",
    "    - get the nearest neighbors of src via the first-order-markov model inferred from the training reader sessions\n",
    "    - check rank of trg among nearest neighbors\n",
    "    '''\n",
    "    lengthwise_rank_list = {}\n",
    "    rank_list = []\n",
    "    skipped_count = 0; missing_sources = 0; missing_targets = 0\n",
    "    for sess_len, (pid_src,pid_trg) in queries:\n",
    "        if type(pid_src) == tuple:\n",
    "            if markov_order == 1:\n",
    "                query = pid_src[-1]\n",
    "            else:\n",
    "                query = pid_src\n",
    "        else:\n",
    "            query = pid_src\n",
    "        skip_query = check_query_skip_status(query, pid_trg, model_list)\n",
    "        if skip_query:\n",
    "            skipped_count += 1\n",
    "        else:\n",
    "            try:\n",
    "                sorted_targets = model[query]\n",
    "                rank = sorted_targets.index(pid_trg)+1\n",
    "            except KeyError:\n",
    "                rank = 1e6\n",
    "                missing_sources+=1\n",
    "            except ValueError:\n",
    "                rank = 1e6\n",
    "                missing_targets +=1\n",
    "            if sess_len not in lengthwise_rank_list:\n",
    "                lengthwise_rank_list[sess_len] = [rank]\n",
    "            else:\n",
    "                lengthwise_rank_list[sess_len].append(rank)\n",
    "            rank_list.append(rank)\n",
    "\n",
    "    print(f'#Skipped-queries = {skipped_count}; #Missing-sources = {missing_sources}; #Missing-targets = {missing_targets}')\n",
    "    return np.array(rank_list), lengthwise_rank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pruned_and_write_results(path_type, minlen, num_sessions, queries, model_list, model, model_order, query_order, results_fname, lenresults_fname):\n",
    "    if not os.path.isfile(results_fname):\n",
    "        fout = open(results_fname, 'w'); fout_len = open(lenresults_fname, 'w')\n",
    "        fout.write(\"PathType\\tMinSessionLength\\tMarkovOrder\\tQueryOrder\\t#Sessions\\t#Queries\\t#Queries_Predicted\\tRecall@1\\tRecall@5\\tRecall@10\\tRecall@50\\tRecall@100\\tMRR\\n\")\n",
    "        fout_len.write(\"PathType\\tMinSessionLength\\tMarkovOrder\\tQueryOrder\\t#Sessions\\tQuerySessionLength\\t#Queries\\t#Queries_Predicted\\tRecall@1\\tRecall@5\\tRecall@10\\tRecall@50\\tRecall@100\\tMRR\\n\")\n",
    "    else:\n",
    "        fout = open(results_fname, 'a'); fout_len = open(lenresults_fname, 'a')\n",
    "\n",
    "    ranks_queries, lengthwise_ranks_queries = queriesRanksNoMissing(queries, model_list, model, model_order)\n",
    "    nqueries, recall1, recall5, recall10, recall50, recall100, mrr = metrics(ranks_queries)\n",
    "    fout.write(f'{path_type}\\t{minlen}\\t{model_order}\\t{query_order}\\t{num_sessions}\\t{len(queries)}\\t{nqueries}\\t{recall1}\\t{recall5}\\t{recall10}\\t{recall50}\\t{recall100}\\t{mrr}\\n')\n",
    "    for session_length in sorted(lengthwise_ranks_queries):\n",
    "        rank_list = lengthwise_ranks_queries[session_length]\n",
    "        nqueries, recall1, recall5, recall10, recall50, recall100, mrr = metrics(np.array(rank_list))\n",
    "        fout_len.write(f'{path_type}\\t{minlen}\\t{model_order}\\t{query_order}\\t{num_sessions}\\t{session_length}\\t{len(queries)}\\t{nqueries}\\t{recall1}\\t{recall5}\\t{recall10}\\t{recall50}\\t{recall100}\\t{mrr}\\n')\n",
    "    fout.close()\n",
    "    fout_len.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_and_write_results(path_type, minlen, num_sessions, queries, model, model_order, query_order, results_fname, lenresults_fname):\n",
    "    if not os.path.isfile(results_fname):\n",
    "        fout = open(results_fname, 'w'); fout_len = open(lenresults_fname, 'w')\n",
    "        fout.write(\"PathType\\tMinSessionLength\\tMarkovOrder\\tQueryOrder\\t#Sessions\\t#Queries\\t#Queries_Predicted\\tRecall@1\\tRecall@5\\tRecall@10\\tRecall@50\\tRecall@100\\tMRR\\n\")\n",
    "        fout_len.write(\"PathType\\tMinSessionLength\\tMarkovOrder\\tQueryOrder\\t#Sessions\\tQuerySessionLength\\t#Queries\\t#Queries_Predicted\\tRecall@1\\tRecall@5\\tRecall@10\\tRecall@50\\tRecall@100\\tMRR\\n\")\n",
    "    else:\n",
    "        fout = open(results_fname, 'a'); fout_len = open(lenresults_fname, 'a')\n",
    "\n",
    "    ranks_queries, lengthwise_ranks_queries = queriesRanks(queries, model, markov_order=model_order)\n",
    "    nqueries, recall1, recall5, recall10, recall50, recall100, mrr = metrics(ranks_queries)\n",
    "    fout.write(f'{path_type}\\t{minlen}\\t{model_order}\\t{query_order}\\t{num_sessions}\\t{len(queries)}\\t{nqueries}\\t{recall1}\\t{recall5}\\t{recall10}\\t{recall50}\\t{recall100}\\t{mrr}\\n')\n",
    "    for session_length in sorted(lengthwise_ranks_queries):\n",
    "        rank_list = lengthwise_ranks_queries[session_length]\n",
    "        nqueries, recall1, recall5, recall10, recall50, recall100, mrr = metrics(np.array(rank_list))\n",
    "        fout_len.write(f'{path_type}\\t{minlen}\\t{model_order}\\t{query_order}\\t{num_sessions}\\t{session_length}\\t{len(queries)}\\t{nqueries}\\t{recall1}\\t{recall5}\\t{recall10}\\t{recall50}\\t{recall100}\\t{mrr}\\n')\n",
    "    fout.close()\n",
    "    fout_len.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(mrr_list):\n",
    "    mrr = np.mean(1/mrr_list)\n",
    "    recall1 = np.where((mrr_list <= 1) & (mrr_list != 1e6))[0].shape[0]/mrr_list.shape[0]\n",
    "    recall5 = np.where((mrr_list <= 5) & (mrr_list != 1e6))[0].shape[0]/mrr_list.shape[0]\n",
    "    recall10 = np.where((mrr_list <= 10) & (mrr_list != 1e6))[0].shape[0]/mrr_list.shape[0]\n",
    "    recall50 = np.where((mrr_list <= 50) & (mrr_list != 1e6))[0].shape[0]/mrr_list.shape[0]\n",
    "    recall100 = np.where((mrr_list <= 100) & (mrr_list != 1e6))[0].shape[0]/mrr_list.shape[0]\n",
    "    return mrr_list.shape[0], recall1, recall5, recall10, recall50, recall100, mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_paths_random_shuffle(paths_fname):\n",
    "    paths_full = []\n",
    "    for line in tqdm(open(paths_fname)):\n",
    "        line = line.strip().split(\",\")\n",
    "        paths_full.append(line[1])\n",
    "    tsplit = time.time()\n",
    "    random.shuffle(paths_full)\n",
    "    num_train = int(0.8 * len(paths_full))\n",
    "    num_val = int(0.1* len(paths_full))\n",
    "    num_test = len(paths_full) - num_train - num_val\n",
    "    print(num_train, num_val, num_test)\n",
    "    paths_train = paths_full[0:num_train]; paths_val = paths_full[num_train:num_train+num_val]; paths_test = paths_full[num_train+num_val:]\n",
    "    print(f'Splitting using random shuffle took {time.time()-tsplit} seconds')\n",
    "    return paths_train, paths_val, paths_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(paths_full):\n",
    "    paths_train, paths_test = train_test_split(paths_full, test_size=0.2, random_state=42)\n",
    "    paths_val, paths_test = train_test_split(paths_test, test_size=0.5, random_state=42)\n",
    "    return paths_train, paths_val, paths_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_paths(paths_fname):\n",
    "    paths_full = []\n",
    "    for lnum, line in enumerate(tqdm(open(paths_fname))):\n",
    "        if lnum == 0:\n",
    "            continue\n",
    "        line = line.strip().split(\",\")\n",
    "        paths_full.append(line[1])\n",
    "    return paths_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(os.path.join(os.getcwd(),os.pardir))\n",
    "PATH_IN = root_dir\n",
    "langlist = ['en', 'ru', 'ja', 'de', 'fr', 'it', 'pl', 'fa']\n",
    "path_types = ['real_nav', 'gen_clickstream_private', 'gen_clickstream_public', 'gen_graph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in langlist:\n",
    "    print(f'{lang}wiki')\n",
    "    out_dir = os.path.join(PATH_IN, 'downstream_tasks', 'next_article_results', lang)\n",
    "    if os.path.isdir(out_dir):\n",
    "        shutil.rmtree(out_dir)\n",
    "    os.makedirs(out_dir)\n",
    "    model = {}\n",
    "    for path_type in path_types:\n",
    "        model[path_type] = {}\n",
    "        print(f'Paths: {path_type}')\n",
    "        paths_fname = os.path.join(PATH_IN, 'data', 'navigation_paths', lang, f'paths_{path_type}.csv')\n",
    "        \n",
    "        tread = time.time()\n",
    "        paths_full = read_paths(paths_fname)\n",
    "        if path_type == 'real_nav':\n",
    "            paths_train, paths_val, paths_test = train_val_test_split(paths_full)\n",
    "        else:\n",
    "            paths_train, _, _ = train_val_test_split(paths_full)\n",
    "        print(len(paths_train), len(paths_val), len(paths_test))\n",
    "        print(f'Reading {paths_fname} took {time.time()-tread} seconds')\n",
    "\n",
    "        queries_dict = {}\n",
    "        for minlen in [2,3]:\n",
    "            model[path_type][minlen] = {}; queries_dict[minlen] = {}\n",
    "            for order in [1,2]:\n",
    "                if order == minlen:\n",
    "                    continue\n",
    "                model_out_fname = os.path.join(PATH_IN, 'data', 'models', lang, f\"markov_model_{path_type}_minlen{minlen}_order{order}.pickle\")\n",
    "                results_fname = os.path.join(out_dir, f\"{lang}wiki_nextarticle_prediction_minlen{minlen}_modelorder{order}_queryorder{order}.tsv\")\n",
    "                lenresults_fname = os.path.join(out_dir, f\"{lang}wiki_nextarticle_prediction_minlen{minlen}_modelorder{order}_queryorder{order}_lengthwise.tsv\")\n",
    "\n",
    "                tmodel = time.time()\n",
    "                transition_probs, num_sessions = prepare_transition_probs(paths_train, markov_order=order, min_session_len=minlen)\n",
    "                print(f'Building ProbMap for Markov order = {order} with sessions of length at least {minlen} took {time.time()-tmodel} seconds')\n",
    "                model[path_type][minlen][order] = (transition_probs, num_sessions)\n",
    "                save_model(transition_probs, model_out_fname)\n",
    "\n",
    "                tqueryprep = time.time()\n",
    "                queries, num_sessions = prepare_queries(paths_test, -1, markov_order=order, min_session_len=minlen)\n",
    "                queries_dict[minlen][order] = (queries, num_sessions)\n",
    "                print(f'Prepared {len(queries)} queries from {num_sessions} sessions in {time.time()-tqueryprep} seconds')\n",
    "                \n",
    "                tquery = time.time()\n",
    "                query_and_write_results(path_type, minlen, num_sessions, queries, transition_probs, order, order, results_fname, lenresults_fname)\n",
    "                print(f'Querying model with markov_order={order} for {len(queries)} queries with min session length {minlen} took {time.time() - tquery} seconds')\n",
    "\n",
    "        results_fname = os.path.join(out_dir, f\"{lang}wiki_nextarticle_prediction_minlen{minlen}_modelorder1_queryorder2.tsv\")\n",
    "        lenresults_fname = os.path.join(out_dir, f\"{lang}wiki_nextarticle_prediction_minlen{minlen}_modelorder1_queryorder2_lengthwise.tsv\")\n",
    "        tquery = time.time()\n",
    "        query_and_write_results(path_type, 3, queries_dict[3][2][1], queries_dict[3][2][0], model[path_type][3][1][0], 1, 2, results_fname, lenresults_fname)\n",
    "        print(f'Querying model with markov_order=1 for {len(queries_dict[3][2][0])} second-order queries with min session length 3 took {time.time() - tquery} seconds')\n",
    "\n",
    "    print(f'{lang}wiki pruned queries')\n",
    "    for minlen, model_order, query_order in [(2,1,1), (3,1,1), (3,1,2), (3,2,2)]:\n",
    "        tload_start = time.time()\n",
    "        model_list = []\n",
    "        for path_type in path_types:\n",
    "            if path_type != 'gen_graph':\n",
    "                model_list.append(model[path_type][minlen][model_order][0])\n",
    "        tload_end = time.time()\n",
    "\n",
    "        for path_type in path_types:\n",
    "            results_fname = os.path.join(out_dir, f\"{lang}wiki_nextarticle_prediction_prunedqueries_minlen{minlen}_modelorder{model_order}_queryorder{query_order}.tsv\")\n",
    "            lenresults_fname = os.path.join(out_dir, f\"{lang}wiki_nextarticle_prediction_prunedqueries_minlen{minlen}_modelorder{model_order}_queryorder{query_order}_lengthwise.tsv\")\n",
    "            tquery = time.time()\n",
    "            query_pruned_and_write_results(path_type, minlen, queries_dict[minlen][query_order][1], queries_dict[minlen][query_order][0], model_list, model[path_type][minlen][model_order][0], model_order, query_order, results_fname, lenresults_fname)\n",
    "            print(f'Querying model with markov_order={model_order} for {len(queries_dict[minlen][query_order][0])} queries of query_order={query_order} with min session length {minlen} took {(tload_end - tload_start) + time.time() - tquery} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
