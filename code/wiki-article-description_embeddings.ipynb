{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import os,sys\n",
    "import json\n",
    "import bz2\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "from tokenizer import _tokenize\n",
    "import pickle\n",
    "import utils\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import mwxml\n",
    "import mwparserfromhell as mwph\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "forbidden_link_prefixes = ['category', 'image', 'file'] ## for english\n",
    "from mwtext.wikitext_preprocessor import WikitextPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "torch.cuda.set_device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(os.path.join(os.getcwd(),os.pardir))\n",
    "PATH_IN = os.path.join(root_dir, 'data', 'xml_dumps')\n",
    "PATH_OUT = os.path.join(root_dir, 'data', 'article_embeddings')\n",
    "langlist = ['en', 'ru', 'ja', 'de', 'fr', 'it', 'pl', 'fa']\n",
    "snapshot = '20210401'\n",
    "N_articles_max = -1 ## maxmimum number of articles to parse (put -1 for all)\n",
    "N_dim = 300 ## number of dimensions for word-vectors (default 300, can be reduced)\n",
    "N_cores_max = 20 ## maximum number of cores to use for parallel parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_fasttext = {}\n",
    "print('loading model')\n",
    "\n",
    "for lang in ['en', 'ru', 'ja', 'de', 'fr', 'it', 'pl', 'fa']:\n",
    "    vectors_fasttext[lang] = utils.loadWordVectors(os.path.join(root_dir, 'data', 'pretrained_embeddings', f'cc.{lang}.300.vec'))\n",
    "\n",
    "print(\"Embeddings Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in ['en', 'ru', 'ja', 'de', 'fr', 'it', 'pl', 'fa']:\n",
    "    print(len(vectors_fasttext[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('xlm-r-distilroberta-base-paraphrase-v1')\n",
    "\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding_xlm(sentences, emb_model):\n",
    "    embedding_array = []\n",
    "    embeddings = emb_model.encode(sentences)\n",
    "    for embedding in embeddings:\n",
    "        embedding_array.append((embedding/norm(embedding)).tolist())\n",
    "    embedding_array = np.array(embedding_array, dtype='float64')\n",
    "    return embedding_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting xml dump chunks downloaded from Internet Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml_chunks_local(lang, wiki, snapshot):\n",
    "    paths = []\n",
    "    dump_fn = os.path.join(PATH_IN, f'{lang}_{snapshot}', f'{wiki}-{snapshot}-pages-articles.xml.bz2')\n",
    "    for infile in glob.glob('{0}/{1}_{3}/{2}-{3}-pages-articles*.xml*.bz2'.format(PATH_IN, lang, wiki,snapshot) ):\n",
    "        if infile == dump_fn:\n",
    "            continue\n",
    "        if 'multistream' in infile:\n",
    "            continue\n",
    "        paths += [infile]\n",
    "    if len(paths) == 0:\n",
    "        paths+=[dump_fn]\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description_embeddings(text_tokens_processed, text_tokens, embeddingType, emb_model):\n",
    "    if embeddingType == 'xlm':\n",
    "        embedding_array = get_sentence_embedding_xlm(text_tokens, emb_model)\n",
    "    else:\n",
    "        embedding_array = utils.get_sentence_embedding(text_tokens_processed, emb_model)\n",
    "    return embedding_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forbidden_link_prefixes = ['category', 'image', 'file'] ## for english\n",
    "wtpp = WikitextPreprocessor(forbidden_link_prefixes)\n",
    "\n",
    "def page_to_vector(dump, path):\n",
    "    for page in dump:\n",
    "        # talk pages for existing articles\n",
    "        if page.namespace == 0 and page.redirect is None:\n",
    "           ## go to most recent revision\n",
    "            for rev in page: pass \n",
    "            ## get wikitext of last revision\n",
    "            wikitext = rev.text\n",
    "            ## get only first section\n",
    "            first_section = re.search('={2,}.+?={2,}', wikitext)\n",
    "            if first_section:\n",
    "                wikitext = wikitext[:first_section.span()[0]]\n",
    "            # concatenate paragpahs as one list of tokens (text)\n",
    "            text = wtpp.process(wikitext)\n",
    "            text_tokens = []; text_tokens_processed = []\n",
    "            for paragraph in text:\n",
    "                paragraph_text  = \" \".join(paragraph)\n",
    "                paragraph_text_without_sw = [word for word in _tokenize(paragraph_text) if not word in all_stopwords]\n",
    "                text_tokens.append(paragraph_text)\n",
    "                text_tokens_processed += paragraph_text_without_sw\n",
    "\n",
    "            dict_page = {\n",
    "                'page_id':page.id, \n",
    "                'rev_id':rev.id, \n",
    "                'page_title':page.title, \n",
    "                'text_tokens': text_tokens,\n",
    "                'text_tokens_processed': text_tokens_processed\n",
    "            }\n",
    "            yield dict_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_article_descriptions(files, wiki, snapshot):\n",
    "    N_articles = 0\n",
    "    N_articles_kept = 0\n",
    "    threads = min([N_cores_max, len(files)])\n",
    "    print(f'Threads = {threads}')\n",
    "\n",
    "    print(f'processing dump {wiki}-{snapshot}')\n",
    "    t1 = time.time()\n",
    "\n",
    "    fout = bz2.open(os.path.join(PATH_OUT, f'article-descriptions_{wiki}-{snapshot}.jsonl.bz2'), 'wt')\n",
    "\n",
    "    for dict_page in mwxml.map(page_to_vector, files, threads=threads):    \n",
    "        fout.write(json.dumps(dict_page) + '\\n')\n",
    "        N_articles_kept += 1\n",
    "        if N_articles_kept%100000==0:\n",
    "            print('... processed %s articles in %.2f'%(N_articles_kept,time.time()-t1))\n",
    "        if N_articles_kept==N_articles_max:\n",
    "            break\n",
    "    t2 = time.time()\n",
    "    print('done in %s seconds'%( t2-t1))\n",
    "\n",
    "    fout.close()\n",
    "    return N_articles_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_embeddings(wiki, snapshot, embeddingType, emb_model):\n",
    "    N_articles_read = 0\n",
    "    ferr = open(os.path.join(PATH_OUT, f'{wiki}-{snapshot}_article_description_{embeddingType}_embeddings.error'), \"w\")\n",
    "\n",
    "    entity_embeddings = {}\n",
    "    num_missing_articles = 0\n",
    "\n",
    "    print(f'Computing {embeddingType} embeddings for {wiki}-{snapshot}')\n",
    "    t1 = time.time()\n",
    "\n",
    "    xlm_embeddings_for_pca = []\n",
    "    with bz2.open(os.path.join(PATH_OUT, f'article-descriptions_{wiki}-{snapshot}.jsonl.bz2'), 'rt') as fin:\n",
    "        for line in fin:\n",
    "            article_obj = json.loads(line)\n",
    "            page_id = article_obj['page_id']\n",
    "            rev_id = article_obj['rev_id']\n",
    "            page_title = article_obj['page_title']\n",
    "            text_tokens_processed = article_obj['text_tokens_processed']\n",
    "            text_tokens = article_obj['text_tokens']\n",
    "\n",
    "            embedding_array = get_description_embeddings(text_tokens_processed, text_tokens, embeddingType, emb_model)\n",
    "            if embedding_array.size == 0: # None of the description words had an embedding\n",
    "                ferr.write(f'No embeddings in {embeddingType} for {page_id}, {rev_id}, {page_title}, with text: {text_tokens} \\n')\n",
    "                num_missing_articles += 1\n",
    "                continue\n",
    "            sent_embedding = np.average(embedding_array, axis=0)\n",
    "            sent_embedding /= norm(sent_embedding)\n",
    "            entity_embeddings[page_id] = sent_embedding\n",
    "            if embeddingType == 'xlm' and random.uniform(0,1)<0.16:\n",
    "                xlm_embeddings_for_pca.append(sent_embedding.tolist())\n",
    "\n",
    "            N_articles_read += 1\n",
    "                if N_articles_read%100000==0:\n",
    "                print('... processed %s articles in %.2f'%(N_articles_read,time.time()-t1))\n",
    "            if N_articles_read==N_articles_max:\n",
    "                break\n",
    "\n",
    "    t2 = time.time()\n",
    "    print('done in %s seconds'%( t2-t1))\n",
    "\n",
    "    try:\n",
    "        ferr.close()\n",
    "    except:\n",
    "        print('Error file already closed!')\n",
    "    \n",
    "    return num_missing_articles, entity_embeddings, xlm_embeddings_for_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langlist = ['en', 'ru', 'ja', 'de', 'fr', 'it', 'pl', 'fa']\n",
    "N_articles_max = -1\n",
    "\n",
    "for lang in langlist:\n",
    "    wiki = f'{lang}wiki'\n",
    "    if lang == 'en':\n",
    "        all_stopwords = stopwords.words('english')\n",
    "        all_stopwords.remove('not')\n",
    "    elif lang == 'de':\n",
    "        all_stopwords = stopwords.words('german')\n",
    "    elif lang == 'fr':\n",
    "        all_stopwords = stopwords.words('french')\n",
    "    elif lang == 'ru':\n",
    "        all_stopwords = stopwords.words('russian')\n",
    "    elif lang == 'it':\n",
    "        all_stopwords = stopwords.words('italian')\n",
    "    else:\n",
    "        all_stopwords = []\n",
    "\n",
    "    paths = get_xml_chunks_local(lang, wiki, snapshot)\n",
    "    print(wiki, len(paths))\n",
    "    num_articles = save_article_descriptions(paths, wiki, snapshot)\n",
    "    print(f'There are {num_articles} articles in {wiki}-{snapshot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embeddingType in ['fasttext']:\n",
    "    for lang in langlist:\n",
    "        wiki = f'{lang}wiki'\n",
    "\n",
    "        if embeddingType == 'fasttext':\n",
    "            model_dict = vectors_fasttext[lang]\n",
    "        else:\n",
    "            model_dict = model\n",
    "        num_missing_articles, entity_embeddings, xlm_embeddings_for_pca = get_article_embeddings(wiki, snapshot, embeddingType, model_dict)\n",
    "        print(f'There are {num_missing_articles} articles in {wiki} with missing embeddings for {embeddingType}')\n",
    "        pickle.dump(entity_embeddings, open(os.path.join(PATH_OUT, f'article-description-embeddings_{wiki}-{snapshot}-{embeddingType}.pickle'), \"wb\"))\n",
    "        print(len(xlm_embeddings_for_pca))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
